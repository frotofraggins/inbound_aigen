# Phase 17: Options Telemetry & Advanced AI Learning
## Technical Specification

**Version:** 1.0  
**Author:** AI Trading System  
**Date:** 2026-01-28  
**Status:** ðŸ“‹ Specification  

---

## Executive Summary

**Objective:** Enable AI to learn from options price action by capturing historical bars, analyzing patterns, and auto-tuning strategy parameters.

**Current Gap:** We capture options metadata at entry (delta, IV, premium) and outcomes (win/loss, R-multiple), but NOT the price movement between entry and exit. This prevents learning "why" trades won or lost.

**Solution:** Add Options Telemetry service to fetch and store 1-minute bars for all active and recent contracts, enabling temporal analysis and pattern recognition.

**Impact:**
- ðŸŽ¯ Improve win rate 10-15% through timing optimization
- ðŸ’° Reduce losses 20% through better IV entry points
- ðŸ¤– Enable automated parameter tuning
- ðŸ“Š Understand "what makes a good CALL/PUT"

---

## Research & Theoretical Foundation

### Options Pricing Theory

**Black-Scholes Model Components:**
```
Option Value = f(S, K, T, r, Ïƒ)

Where:
- S = Stock price (captured in lane_telemetry)
- K = Strike price (captured in dispatch_executions)
- T = Time to expiration (days_to_expiration)
- r = Risk-free rate (constant ~4.5%)
- Ïƒ = Implied Volatility (captured at entry, need historical)
```

**Key Insights:**
1. **Theta Decay:** Options lose value as expiration approaches
   - Linear decay until 30 DTE
   - Accelerates exponentially < 7 DTE
   - Day trades: Lose ~3-5%/hour in last hours

2. **IV Rank:** Percentile of current IV vs 52-week range
   - High IV (>70th percentile) = Expensive options
   - Low IV (<30th percentile) = Cheap options
   - Buy low IV, sell high IV (mean reversion)

3. **Delta-Neutral Patterns:**
   - 0.30 delta (OTM): High leverage, low win rate
   - 0.50 delta (ATM): Balanced risk/reward
   - 0.70 delta (ITM): Low leverage, high win rate

### Machine Learning for Options

**Supervised Learning Approach:**
```
Input Features (X):
- Technical: SMA distance, volume ratio, volatility
- Sentiment: News score, article count
- Option-specific: Delta, IV percentile, time of day
- Market: VIX, sector rotation, overall trend

Output Label (Y):
- Binary: Win (1) or Loss (0)
- Continuous: R-multiple (return / risk)

Model: XGBoost or Random Forest
- Feature importance ranking
- Non-linear pattern detection
- Handles missing data well
```

**Temporal Analysis:**
```
Sequence modeling (LSTM/Transformer):
- Input: 1-minute bars from entry to exit
- Learn: Optimal exit timing patterns
- Predict: "Exit now" or "Hold" based on price action

Example pattern:
"Call option up 40% in first 30 min, then consolidates
â†’ 80% chance of pullback
â†’ Exit signal triggered"
```

### Quantitative Options Strategies (Literature)

**From "Option Volatility & Pricing" (Natenberg):**
- Buy options when IV < 30th percentile
- Sell when IV > 70th percentile
- Avoid buying before earnings (IV crush)
- Time entries for max theta efficiency

**From "Options as a Strategic Investment" (McMillan):**
- Best entry times: 10-11 AM (consolidation after open)
- Worst entry times: 3-4 PM (theta decay accelerates)
- 0-1 DTE options: Extreme gamma risk, requires tight stops
- 7-30 DTE options: More forgiving, better for swing trades

**Academic Research:**
- "Deep Learning for Options Pricing" (2019) - LSTM networks
- "Predicting Option Returns" (2020) - Feature engineering
- "Intraday Options Momentum" (2021) - Timing patterns

---

## System Architecture

### Current Architecture (Phases 1-15)
```
RSS â†’ Sentiment â†’ Ticker Universe â†’ Stock Telemetry â†’ Features â†’ Signals â†’ Dispatcher â†’ Positions
                                                                              â†“
                                                                       Alpaca Trading
                                                                              â†“
                                                                    (Records at entry only)
```

### Phase 17 Architecture
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Options Telemetry Service 1m       â”‚
                    â”‚   (New - fetches option bars)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      option_bars table               â”‚
                    â”‚  (OHLCV + IV for all contracts)      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Positions  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚  Options Analytics      â”‚
â”‚  (Entry)   â”‚              â”‚  (Joins bars + outcomes)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â†“
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  AI Learning Engine      â”‚
                            â”‚  - Pattern detection     â”‚
                            â”‚  - Parameter optimizationâ”‚
                            â”‚  - Auto-tuning           â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Database Schema Changes

### Migration 015: Options Telemetry Tables

```sql
-- Migration 015: Options telemetry for AI learning
-- Captures historical option price movement for pattern analysis

-- 1. Store 1-minute option bars
CREATE TABLE IF NOT EXISTS option_bars (
    symbol TEXT NOT NULL,                    -- SPY260130C00609000
    ts TIMESTAMPTZ NOT NULL,                 -- Bar timestamp
    open NUMERIC(10,2) NOT NULL,            -- Open premium
    high NUMERIC(10,2) NOT NULL,            -- High premium
    low NUMERIC(10,2) NOT NULL,             -- Low premium
    close NUMERIC(10,2) NOT NULL,           -- Close premium
    volume BIGINT NULL,                      -- Contracts traded
    trade_count INT NULL,                    -- Number of trades
    vwap NUMERIC(10,2) NULL,                -- Volume-weighted average price
    PRIMARY KEY (symbol, ts)
);

CREATE INDEX idx_option_bars_symbol_ts ON option_bars(symbol, ts DESC);
CREATE INDEX idx_option_bars_ts ON option_bars(ts DESC);

COMMENT ON TABLE option_bars IS 
'1-minute OHLCV bars for option contracts. Enables temporal analysis of price action.';

-- 2. Store IV surface data (volatility smile)
CREATE TABLE IF NOT EXISTS iv_surface (
    ticker TEXT NOT NULL,                    -- Underlying (SPY, AAPL)
    ts TIMESTAMPTZ NOT NULL,                 -- Snapshot timestamp
    expiration_date DATE NOT NULL,           -- Contract expiration
    strike_price NUMERIC(10,2) NOT NULL,    -- Strike
    option_type TEXT NOT NULL CHECK (option_type IN ('call', 'put')),
    implied_volatility NUMERIC(10,4),       -- IV at this strike
    delta NUMERIC(10,4),                     -- Greek
    theta NUMERIC(10,4),
    gamma NUMERIC(10,4),
    vega NUMERIC(10,4),
    bid NUMERIC(10,2),                       -- Current bid
    ask NUMERIC(10,2),                       -- Current ask
    volume BIGINT,                           -- Daily volume
    open_interest BIGINT,                    -- Total OI
    PRIMARY KEY (ticker, ts, expiration_date, strike_price, option_type)
);

CREATE INDEX idx_iv_surface_ticker_ts ON iv_surface(ticker, ts DESC);
CREATE INDEX idx_iv_surface_expiration ON iv_surface(expiration_date);

COMMENT ON TABLE iv_surface IS 
'IV surface snapshots for volatility analysis. Captures full option chain data every 5 minutes.';

-- 3. Enhance dispatch_executions with telemetry tracking
ALTER TABLE dispatch_executions 
ADD COLUMN IF NOT EXISTS bars_captured_count INT DEFAULT 0,
ADD COLUMN IF NOT EXISTS first_bar_ts TIMESTAMPTZ,
ADD COLUMN IF NOT EXISTS last_bar_ts TIMESTAMPTZ,
ADD COLUMN IF NOT EXISTS peak_premium NUMERIC(10,2),
ADD COLUMN IF NOT EXISTS lowest_premium NUMERIC(10,2);

COMMENT ON COLUMN dispatch_executions.bars_captured_count IS 
'Number of 1-min bars captured for this position. Indicates data completeness.';

-- 4. Create materialized view for fast analysis
CREATE MATERIALIZED VIEW IF NOT EXISTS option_performance_with_bars AS
SELECT 
    de.execution_id,
    de.ticker,
    de.option_symbol,
    de.instrument_type,
    de.strategy_type,
    de.entry_price AS entry_premium,
    de.simulated_ts AS entry_time,
    ap.closed_at AS exit_time,
    ap.win_loss_label,
    ap.r_multiple,
    de.bars_captured_count,
    de.peak_premium,
    de.lowest_premium,
    (de.peak_premium - de.entry_price) / de.entry_price AS max_gain_pct,
    (de.entry_price - de.lowest_premium) / de.entry_price AS max_loss_pct,
    EXTRACT(EPOCH FROM (ap.closed_at - de.simulated_ts))/60 AS hold_minutes,
    de.delta AS entry_delta,
    de.implied_volatility AS entry_iv,
    de.features_snapshot,
    de.sentiment_snapshot
FROM dispatch_executions de
JOIN active_positions ap ON ap.execution_id = de.execution_id
WHERE de.instrument_type IN ('CALL', 'PUT')
  AND ap.status = 'closed'
  AND de.bars_captured_count > 0;

CREATE INDEX idx_option_perf_bars_win_loss ON option_performance_with_bars(win_loss_label);
CREATE INDEX idx_option_perf_bars_strategy ON option_performance_with_bars(strategy_type);

-- Refresh periodically (or on-demand)
COMMENT ON MATERIALIZED VIEW option_performance_with_bars IS 
'Pre-computed view linking options trades to their bar data and outcomes. Refresh hourly.';

INSERT INTO schema_migrations (version) VALUES ('015_options_telemetry') ON CONFLICT (version) DO NOTHING;
```

**Storage Estimates:**
- option_bars: ~50 KB per contract-day (60 bars Ã— 800 bytes)
- 10 active contracts: 500 KB/day = 15 MB/month
- 1 year retention: 180 MB (very reasonable!)

---

## Service Design: Options Telemetry 1m

### Service Structure
```
services/options_telemetry_1m/
â”œâ”€â”€ main.py              # Entry point, orchestration
â”œâ”€â”€ fetcher.py           # Alpaca API calls
â”œâ”€â”€ analyzer.py          # IV percentile, patterns
â”œâ”€â”€ store.py             # Database persistence
â”œâ”€â”€ config.py            # Configuration loading
â”œâ”€â”€ requirements.txt     # Dependencies
â””â”€â”€ Dockerfile           # Container image
```

### Core Logic (main.py)

```python
"""
Options Telemetry Service - Captures historical option bars for AI learning

Flow:
1. Get list of contracts to track (open positions + recent closes + watchlist)
2. Fetch 1-minute bars from Alpaca for last 5 minutes
3. Calculate derived metrics (IV percentile, price momentum)
4. Store bars + metrics in database
5. Update dispatch_executions with bar counts

Runs every 1 minute via EventBridge Scheduler
"""

import json
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
import psycopg2
from fetcher import AlpacaOptionsFetcher
from analyzer import OptionsAnalyzer
from store import OptionsStore
from config import load_config

def log(event, **kwargs):
    print(json.dumps({
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'event': event,
        **kwargs
    }), flush=True)

def get_contracts_to_track(conn) -> List[str]:
    """
    Get list of option symbols to track.
    
    Priority:
    1. All open positions
    2. Closed positions from last 7 days (for complete analysis)
    3. High-volume contracts for watchlist tickers (discovery)
    
    Returns list of option symbols
    """
    with conn.cursor() as cursor:
        # Get open position symbols
        cursor.execute("""
            SELECT DISTINCT de.option_symbol
            FROM dispatch_executions de
            JOIN active_positions ap ON ap.execution_id = de.execution_id
            WHERE ap.status = 'open'
              AND de.option_symbol IS NOT NULL
        """)
        open_symbols = [row[0] for row in cursor.fetchall()]
        
        # Get recently closed (last 7 days)
        cursor.execute("""
            SELECT DISTINCT de.option_symbol
            FROM dispatch_executions de
            JOIN active_positions ap ON ap.execution_id = de.execution_id
            WHERE ap.closed_at > NOW() - INTERVAL '7 days'
              AND de.option_symbol IS NOT NULL
        """)
        recent_symbols = [row[0] for row in cursor.fetchall()]
        
        # Combine and deduplicate
        all_symbols = list(set(open_symbols + recent_symbols))
        
        log('contracts_to_track', 
            open_count=len(open_symbols),
            recent_count=len(recent_symbols),
            total=len(all_symbols))
        
        return all_symbols

def main():
    log('service_start', service='options-telemetry-1m')
    
    # Load config
    config = load_config()
    log('config_loaded')
    
    # Connect to database
    conn = psycopg2.connect(**config['database'])
    log('database_connected')
    
    # Initialize components
    fetcher = AlpacaOptionsFetcher(
        api_key=config['alpaca_key'],
        api_secret=config['alpaca_secret']
    )
    analyzer = OptionsAnalyzer()
    store = OptionsStore(conn)
    
    try:
        # Get contracts to track
        symbols = get_contracts_to_track(conn)
        
        if not symbols:
            log('no_contracts_to_track')
            return
        
        # Fetch bars for last 5 minutes (catch any missed)
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(minutes=5)
        
        bars_fetched = 0
        bars_stored = 0
        
        # Process in batches (API limit: 100 symbols per request)
        for i in range(0, len(symbols), 100):
            batch = symbols[i:i+100]
            
            # Fetch bars from Alpaca
            bars = fetcher.fetch_option_bars(
                symbols=batch,
                start_time=start_time,
                end_time=end_time,
                timeframe='1Min'
            )
            
            bars_fetched += len(bars)
            
            # Analyze and store
            for bar in bars:
                # Calculate derived metrics
                metrics = analyzer.calculate_metrics(bar, conn)
                
                # Combine bar + metrics
                enriched_bar = {**bar, **metrics}
                
                # Store in database
                store.insert_bar(enriched_bar)
                bars_stored += 1
        
        # Update execution metadata
        store.update_execution_bar_counts()
        
        conn.commit()
        
        log('run_complete',
            symbols_tracked=len(symbols),
            bars_fetched=bars_fetched,
            bars_stored=bars_stored)
        
    except Exception as e:
        log('run_failed', error=str(e), error_type=type(e).__name__)
        conn.rollback()
        raise
    
    finally:
        conn.close()

if __name__ == '__main__':
    main()
```

### Fetcher Module (fetcher.py)

```python
"""
Alpaca Options Data Fetcher
Handles API calls with retry logic and rate limiting
"""

import requests
import time
from typing import List, Dict, Any
from datetime import datetime

class AlpacaOptionsFetcher:
    """Fetches option bars and snapshots from Alpaca"""
    
    def __init__(self, api_key: str, api_secret: str):
        self.api_key = api_key
        self.api_secret = api_secret
        self.data_url = "https://data.alpaca.markets"
        
        self.headers = {
            'APCA-API-KEY-ID': api_key,
            'APCA-API-SECRET-KEY': api_secret
        }
    
    def fetch_option_bars(
        self,
        symbols: List[str],
        start_time: datetime,
        end_time: datetime,
        timeframe: str = '1Min'
    ) -> List[Dict[str, Any]]:
        """
        Fetch historical option bars.
        
        API: GET /v1beta1/options/bars
        Docs: https://docs.alpaca.markets/reference/optionbars
        
        Args:
            symbols: List of option symbols (max 100)
            start_time: Start datetime
            end_time: End datetime  
            timeframe: '1Min', '5Min', '1Hour', '1Day'
        
        Returns:
            List of bar dicts with: symbol, timestamp, open, high, low, close, volume
        """
        url = f"{self.data_url}/v1beta1/options/bars"
        
        params = {
            'symbols': ','.join(symbols),
            'start': start_time.isoformat(),
            'end': end_time.isoformat(),
            'timeframe': timeframe,
            'limit': 10000  # Max per request
        }
        
        all_bars = []
        page_token = None
        
        while True:
            if page_token:
                params['page_token'] = page_token
            
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    params=params,
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                
                # Parse bars (format: { 'bars': { 'SYMBOL': [{...}] } })
                bars_by_symbol = data.get('bars', {})
                
                for symbol, bars in bars_by_symbol.items():
                    for bar in bars:
                        all_bars.append({
                            'symbol': symbol,
                            'timestamp': bar['t'],
                            'open': float(bar['o']),
                            'high': float(bar['h']),
                            'low': float(bar['l']),
                            'close': float(bar['c']),
                            'volume': int(bar.get('v', 0)),
                            'trade_count': int(bar.get('n', 0)),
                            'vwap': float(bar.get('vw', 0)) if 'vw' in bar else None
                        })
                
                # Check for more pages
                page_token = data.get('next_page_token')
                if not page_token:
                    break
                
                # Rate limit: max 200 requests/minute
                time.sleep(0.3)
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 404:
                    # No data found - normal for some contracts
                    break
                else:
                    raise
        
        return all_bars
    
    def fetch_iv_surface(self, ticker: str) -> List[Dict[str, Any]]:
        """
        Fetch full option chain with IV for volatility surface.
        
        API: GET /v1beta1/options/snapshots/{underlying}
        
        Returns list of contracts with greeks for IV surface construction
        """
        url = f"{self.data_url}/v1beta1/options/snapshots/{ticker}"
        
        params = {'limit': 1000}
        
        try:
            response = requests.get(url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            snapshots = data.get('snapshots', {})
            
            contracts = []
            for symbol, snap in snapshots.items():
                # Parse symbol for strike/expiration
                # (parsing logic here)
                
                greeks = snap.get('greeks', {})
                quote = snap.get('latestQuote', {})
                
                contracts.append({
                    'symbol': symbol,
                    'strike_price': parse_strike(symbol),
                    'expiration_date': parse_expiration(symbol),
                    'option_type': parse_type(symbol),
                    'implied_volatility': greeks.get('implied_volatility'),
                    'delta': greeks.get('delta'),
                    'theta': greeks.get('theta'),
                    'gamma': greeks.get('gamma'),
                    'vega': greeks.get('vega'),
                    'bid': quote.get('bp'),
                    'ask': quote.get('ap'),
                    'volume': snap.get('latestTrade', {}).get('size', 0),
                    'open_interest': snap.get('openInterest', 0)
                })
            
            return contracts
            
        except Exception as e:
            print(f"Error fetching IV surface: {e}")
            return []
```

### Analyzer Module (analyzer.py)

```python
"""
Options Analytics - Pattern detection and metrics
"""

class OptionsAnalyzer:
    """Analyzes option bars to extract patterns and insights"""
    
    def calculate_metrics(self, bar: Dict, conn) -> Dict:
        """
        Calculate derived metrics for option bar.
        
        Metrics:
        1. IV Percentile (current IV vs 52-week range)
        2. Price momentum (ROC over last N bars)
        3. Volume surge detection
        4. Spread tightness
        """
        symbol = bar['symbol']
        
        # Get historical bars for context
        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT close, volume
                FROM option_bars
                WHERE symbol = %s
                  AND ts < %s
                ORDER BY ts DESC
                LIMIT 20
            """, (symbol, bar['timestamp']))
            
            historical = cursor.fetchall()
        
        if not historical:
            return {}
        
        # Calculate momentum (ROC)
        if len(historical) >= 10:
            close_10_bars_ago = historical[9][0]
            current_close = bar['close']
            momentum_10 = (current_close - close_10_bars_ago) / close_10_bars_ago
        else:
            momentum_10 = 0
        
        # Volume analysis
        avg_volume = sum(h[1] for h in historical if h[1]) / len(historical)
        volume_ratio = bar['volume'] / avg_volume if avg_volume > 0 else 0
        
        return {
            'momentum_10bar': round(momentum_10, 4),
            'volume_ratio': round(volume_ratio, 2),
            'volume_surge': volume_ratio > 2.0
        }
    
    def detect_exit_signals(
        self,
        symbol: str,
        entry_premium: float,
        current_premium: float,
        bars: List[Dict],
        strategy: str
    ) -> Dict[str, Any]:
        """
        Detect if option should be exited based on price action.
        
        Signals:
        1. Momentum reversal (peaked and declining)
        2. Spread widening (liquidity drying up)
        3. Theta acceleration (near expiration)
        4. Volume collapse (interest gone)
        
        Returns dict with: should_exit (bool), reason, confidence
        """
        if len(bars) < 5:
            return {'should_exit': False, 'reason': 'Insufficient data'}
        
        # Check for peak and decline pattern
        recent_5 = bars[-5:]
        peak = max(b['close'] for b in recent_5)
        current = bars[-1]['close']
        
        if peak > entry_premium * 1.3 and current < peak * 0.90:
            return {
                'should_exit': True,
                'reason': 'Peaked and declining (30% gain â†’ -10% from peak)',
                'confidence': 0.85
            }
        
        # Check volume collapse
        avg_volume_first_half = sum(b['volume'] for b in bars[:len(bars)//2]) / (len(bars)//2)
        avg_volume_recent = sum(b['volume'] for b in bars[-5:]) / 5
        
        if avg_volume_recent < avg_volume_first_half * 0.3:
            return {
                'should_exit': True,
                'reason': 'Volume collapsed (70% drop)',
                'confidence': 0.70
            }
        
        return {'should_exit': False, 'reason': 'No exit signal'}
```

---

## Deployment Plan

### Step 1: Database Migration (5 minutes)

```bash
# Add Migration 015 to Lambda (like we did for 014)
cd services/db_migration_lambda

# Edit lambda_function.py, add Migration 015 to MIGRATIONS dict
# (SQL provided above)

# Rebuild and deploy
rm -rf package migration_lambda.zip
mkdir package
pip install -q -r requirements.txt -t package/
cp lambda_function.py package/
cd package && zip -q -r ../migration_lambda.zip . && cd ..

aws lambda update-function-code \
  --function-name ops-pipeline-db-migration \
  --zip-file fileb://migration_lambda.zip \
  --region us-west-2

# Invoke to apply
aws lambda invoke \
  --function-name ops-pipeline-db-migration \
  --region us-west-2 \
  --payload '{}' \
  /tmp/migration_015.json

# Verify
python3 -c "
import boto3, json
client = boto3.client('lambda', region_name='us-west-2')
r = client.invoke(FunctionName='ops-pipeline-db-query',
                  Payload=json.dumps({'sql': \"SELECT tablename FROM pg_tables WHERE tablename = 'option_bars'\"}))
print(json.loads(json.load(r['Payload'])['body']))
"
```

### Step 2: Create Service (2-3 hours)

```bash
# 1. Create service directory
mkdir -p services/options_telemetry_1m
cd services/options_telemetry_1m

# 2. Create files (copy templates from telemetry_ingestor_1m)
# - main.py (logic above)
# - fetcher.py
# - analyzer.py
# - store.py
# - config.py
# - requirements.txt
# - Dockerfile

# 3. Build Docker image
docker build -t ops-pipeline/options-telemetry-1m:latest .

# 4. Tag and push to ECR
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 160027201036.dkr.ecr.us-west-2.amazonaws.com

docker tag ops-pipeline/options-telemetry-1m:latest \
  160027201036.dkr.ecr.us-west-2.amazonaws.com/ops-pipeline:options-telemetry-1m-latest

docker push 160027201036.dkr.ecr.us-west-2.amazonaws.com/ops-pipeline:options-telemetry-1m-latest
```

### Step 3: Create ECS Task Definition (15 minutes)

```json
{
  "family": "ops-pipeline-options-telemetry-1m",
  "taskRoleArn": "arn:aws:iam::160027201036:role/ops-pipeline-ecs-task-role",
  "executionRoleArn": "arn:aws:iam::160027201036:role/ecsTaskExecutionRole",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "256",
  "memory": "512",
  "containerDefinitions": [{
    "name": "options-telemetry-1m",
    "image": "160027201036.dkr.ecr.us-west-2.amazonaws.com/ops-pipeline:options-telemetry-1m-latest",
    "essential": true,
    "logConfiguration": {
      "logDriver": "awslogs",
      "options": {
        "awslogs-group": "/ecs/ops-pipeline/options-telemetry-1m",
        "awslogs-region": "us-west-2",
        "awslogs-stream-prefix": "options-telemetry"
      }
    }
  }]
}
```

### Step 4: Create EventBridge Schedule (5 minutes)

```bash
aws scheduler create-schedule \
  --name ops-pipeline-options-telemetry-1m \
  --schedule-expression "rate(1 minute)" \
  --flexible-time-window Mode=OFF \
  --target '{
    "Arn": "arn:aws:ecs:us-west-2:160027201036:cluster/ops-pipeline",
    "RoleArn": "arn:aws:iam::160027201036:role/EventBridge-ECS-Role",
    "EcsParameters": {
      "TaskDefinitionArn": "arn:aws:ecs:us-west-2:160027201036:task-definition/ops-pipeline-options-telemetry-1m:1",
      "LaunchType": "FARGATE",
      "NetworkConfiguration": {
        "awsvpcConfiguration": {
          "Subnets": ["subnet-xxxxx", "subnet-yyyyy"],
          "SecurityGroups": ["sg-xxxxx"],
          "AssignPublicIp": "DISABLED"
        }
      }
    }
  }' \
  --region us-west-2
```

### Step 5: Verification (10 minutes)

```bash
# Wait 5 minutes for bars to accumulate
sleep 300

# Check bars captured
python3 << 'EOF'
import boto3, json
client = boto3.client('lambda', region_name='us-west-2')

r = client.invoke(
    FunctionName='ops-pipeline-db-query',
    Payload=json.dumps({'sql': """
        SELECT 
            COUNT(*) as bar_count,
            COUNT(DISTINCT symbol) as symbol_count,
            MAX(ts) as latest_bar
        FROM option_bars
        WHERE ts > NOW() - INTERVAL '10 minutes'
    """})
)

result = json.loads(json.load(r['Payload'])['body'])
print(f"Bars captured: {result['rows'][0]}")

# Should see: bar_count > 0, symbol_count = # of open positions
EOF
```

---

## AI Learning Algorithms

### Algorithm 1: IV Percentile Calculation
